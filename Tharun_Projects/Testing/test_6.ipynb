{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_3.5:0.20.3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\Test.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Step 2: Define the schema based on the Pandas DataFrame\n",
    "schema = StructType([\n",
    "    StructField(name, StringType(), True) for name in pandas_df.columns[:-1]  # Assuming the last column is the date column\n",
    "] + [StructField(pandas_df.columns[-1], DateType(), True)])  # Assuming the last column is the date column\n",
    "\n",
    "# Step 3: Convert pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df, schema=schema)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\Test.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Step 2: Exclude columns with the name \"Unnamed\"\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\n",
    "filtered_pandas_df = pandas_df[filtered_columns].dropna()\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path, usecols=[\"Customer\", \"Hierarchy\", \"ParentH1\", \"Site\", \"Commitment Type\", \"Contract Start\", \"Contract End\", \"Summary Year\"])\n",
    "\n",
    "# Step 2: Exclude columns with the name \"Unnamed\"\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\n",
    "filtered_pandas_df = pandas_df[filtered_columns].dropna()\n",
    "\n",
    "# Step 4: Convert 'Contract Start' column to PySpark DateType\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "filtered_pandas_df = filtered_pandas_df.dropna(subset=['Contract Start'])\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Allentown\", \"Pallet Positions\", \"10/31/2016\", \"9/1/2026\", 2026, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 7500, 7500, 7500, 7500, 5967],\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Ft. Worth - Railhead\", \"Pallet Positions\", \"7/1/2016\", \"9/1/2026\", 2026, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 10000, 10000, 10000, 10000, 9733],\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Massillon\", \"Pallet Positions\", \"7/1/2016\", \"9/1/2026\", 2026, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596],\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Massillon - Erie Ave\", \"Pallet Positions\", \"7/1/2016\", \"9/1/2026\", 2026, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11647, 11647, 11647, 11647, 11600]\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"Customer\", \"Hierarchy\", \"ParentH1\", \"Site\", \"Commitment Type\", \"Contract Start\", \"Contract End\", \"Summary Year\"] + \\\n",
    "          [f\"Jan {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Feb {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Mar {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Apr {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"May {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Jun {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Jul {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Aug {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Sep {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Oct {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Nov {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Dec {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Average {year}\" for year in range(2022, 2024)]\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Writing to an Excel file using a raw string\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been written to {excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "\n",
    "# Skip the first 5 rows (0-indexed) and read the data\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\n",
    "filtered_pandas_df = pandas_df[filtered_columns].dropna()\n",
    "\n",
    "# Step 4: Convert 'Contract Start' column to PySpark DateType (Assuming 'Contract Start' is the correct date column)\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "\n",
    "filtered_pandas_df = filtered_pandas_df.dropna(subset=['Contract Start'])\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "\n",
    "# Skip the first 5 rows (0-indexed) and read the data\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and replace null values with '-'\n",
    "filtered_pandas_df = pandas_df[filtered_columns].fillna('-')\n",
    "\n",
    "# Step 4: Convert 'Contract Start' column to PySpark DateType (Assuming 'Contract Start' is the correct date column)\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "\n",
    "filtered_pandas_df = filtered_pandas_df.dropna(subset=['Contract Start'])\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "filtered_pandas_df = pandas_df[filtered_columns].fillna('-')\n",
    "\n",
    "# Step 3: Convert 'Contract Start' column to PySpark DateType (Assuming 'Contract Start' is the correct date column)\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "\n",
    "# Step 4: Replace null values with '-'\n",
    "filtered_pandas_df = filtered_pandas_df.fillna('-')\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "# Step 6: Write to CSV file\n",
    "csv_output_path = r\"C:\\Users\\tharu\\output_data.csv\"\n",
    "filtered_pandas_df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"Data has been written to {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "\n",
    "# Skip the first 5 rows (0-indexed) and read the data\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and replace null values with '-'\n",
    "filtered_pandas_df = pandas_df[filtered_columns].fillna('-')\n",
    "\n",
    "print(filtered_pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provided data\n",
    "data = {\n",
    "    'Customer': ['Kraft Heinz Corp'] * 8 + ['ConAgra Foods Inc'] * 7 + ['Sara Lee'] + ['Lamb Weston'] * 2,\n",
    "    'Hierarchy': ['KRAFTHEINZ'] * 8 + ['CONAGRAPINNACLE'] * 7 + ['SARALEE'] + ['LAMB'] * 2,\n",
    "    'ParentH1': ['KRAFTHEINZ_W'] * 8 + ['CONAGRAPINNACLE_W'] * 7 + ['SARALEE_W'] + ['LAMB_W'] * 2,\n",
    "    'Site': ['Allentown', 'Ft. Worth - Railhead', 'Massillon', 'Massillon - Erie Ave', 'Nampa',\n",
    "             'Ontario OR', 'Rochelle', 'Westgate', 'Atlanta - East Point', 'Carthage', 'Indianapolis',\n",
    "             'Manchester', 'Rochelle Caron', 'Russellville ElMira', 'Victorville', 'Tarboro', 'Westgate', 'Delhi', None],\n",
    "    'Commitment Type': ['Pallet Positions'] * 19\n",
    "}\n",
    "\n",
    "# Ensure all arrays have the same length\n",
    "max_length = max(len(arr) for arr in data.values())\n",
    "data = {key: arr + [None] * (max_length - len(arr)) for key, arr in data.items()}\n",
    "\n",
    "# Schema\n",
    "schema = ['Customer', 'Hierarchy', 'ParentH1', 'Site', 'Commitment Type']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=schema)\n",
    "\n",
    "# Save DataFrame to Excel file with schema\n",
    "df.to_excel(r'C:\\Users\\tharu\\output_data_1.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Customer        Hierarchy           ParentH1  \\\n",
      "0    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "1    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "2    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "3    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "4    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "5    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "6    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "7    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "8   ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "9   ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "10  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "11  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "12  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "13  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "14  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "15           Sara Lee          SARALEE          SARALEE_W   \n",
      "16        Lamb Weston             LAMB             LAMB_W   \n",
      "17        Lamb Weston             LAMB             LAMB_W   \n",
      "18                NaN              NaN                NaN   \n",
      "\n",
      "                    Site   Commitment Type  \n",
      "0              Allentown  Pallet Positions  \n",
      "1   Ft. Worth - Railhead  Pallet Positions  \n",
      "2              Massillon  Pallet Positions  \n",
      "3   Massillon - Erie Ave  Pallet Positions  \n",
      "4                  Nampa  Pallet Positions  \n",
      "5             Ontario OR  Pallet Positions  \n",
      "6               Rochelle  Pallet Positions  \n",
      "7               Westgate  Pallet Positions  \n",
      "8   Atlanta - East Point  Pallet Positions  \n",
      "9               Carthage  Pallet Positions  \n",
      "10          Indianapolis  Pallet Positions  \n",
      "11            Manchester  Pallet Positions  \n",
      "12        Rochelle Caron  Pallet Positions  \n",
      "13   Russellville ElMira  Pallet Positions  \n",
      "14           Victorville  Pallet Positions  \n",
      "15               Tarboro  Pallet Positions  \n",
      "16              Westgate  Pallet Positions  \n",
      "17                 Delhi  Pallet Positions  \n",
      "18                   NaN  Pallet Positions  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df_read = pd.read_excel(r'C:\\Users\\tharu\\output_data_1.xlsx')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
