{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Creating RDD by Using Parallelize() from a Python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[1, 2, 3, 4, 5, 6, 7]\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "l = [1, 2, 3, 4, 5, 6, 7]\n",
    "print(type(l))\n",
    "print(l)\n",
    "rdd_1 = spark.sparkContext.parallelize(l)\n",
    "print(type(rdd_1))\n",
    "print(rdd_1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDD by using textFile() from a text or csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['CustomerID,FirstName,LastName,Email,PhoneNumber,Address,City,State,PostalCode,Country,DateOfBirth', '1,John,Doe,john.doe@example.com,555-1234,123 Elm St,Springfield,IL,62704,USA,1985-03-15', '2,Jane,Smith,jane.smith@example.com,555-5678,456 Oak St,Columbus,OH,43215,USA,1990-06-20', '3,Bob,Johnson,bob.johnson@example.com,555-9876,789 Pine St,Dallas,TX,75201,USA,1982-11-05', '4,Alice,Williams,alice.williams@example.com,555-2468,321 Maple St,Seattle,WA,98101,USA,1988-09-10', '5,Michael,Brown,michael.brown@example.com,555-1357,654 Cedar St,San Francisco,CA,94102,USA,1975-12-25', '6,Emily,Garcia,emily.garcia@example.com,555-3698,987 Birch St,Miami,FL,33101,USA,1995-04-10', '7,David,Martinez,david.martinez@example.com,555-7412,258 Ash St,Denver,CO,80201,USA,1987-07-23', '8,Olivia,Thompson,olivia.thompson@example.com,555-8529,753 Palm St,Phoenix,AZ,85001,USA,1993-02-14', '9,Daniel,Anderson,daniel.anderson@example.com,555-3692,125 Willow St,New York,NY,10001,USA,1980-08-30', '10,Sophia,Lopez,sophia.lopez@example.com,555-9638,789 Elm St,Los Angeles,CA,90001,USA,1992-05-05']\n"
     ]
    }
   ],
   "source": [
    "rdd_2 = spark.sparkContext.textFile(r\"C:\\Users\\tharu\\Project_Git_Push\\New_Tharun_Branch\\Tharun_Projects\\Sample_Data\\data_1.csv\")\n",
    "print(type(rdd_2))\n",
    "print(rdd_2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating RDD from a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7]\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd_3 = rdd_1.filter(lambda x: x >= 2)\n",
    "print(rdd_3.collect())\n",
    "print(type(rdd_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "MapPartitionsRDD[18] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "[Row(CustomerID=1, FirstName='John', LastName='Doe', Email='john.doe@example.com', PhoneNumber='555-1234', Address='123 Elm St', City='Springfield', State='IL', PostalCode=62704, Country='USA', DateOfBirth=datetime.date(1985, 3, 15)), Row(CustomerID=2, FirstName='Jane', LastName='Smith', Email='jane.smith@example.com', PhoneNumber='555-5678', Address='456 Oak St', City='Columbus', State='OH', PostalCode=43215, Country='USA', DateOfBirth=datetime.date(1990, 6, 20)), Row(CustomerID=3, FirstName='Bob', LastName='Johnson', Email='bob.johnson@example.com', PhoneNumber='555-9876', Address='789 Pine St', City='Dallas', State='TX', PostalCode=75201, Country='USA', DateOfBirth=datetime.date(1982, 11, 5)), Row(CustomerID=4, FirstName='Alice', LastName='Williams', Email='alice.williams@example.com', PhoneNumber='555-2468', Address='321 Maple St', City='Seattle', State='WA', PostalCode=98101, Country='USA', DateOfBirth=datetime.date(1988, 9, 10)), Row(CustomerID=5, FirstName='Michael', LastName='Brown', Email='michael.brown@example.com', PhoneNumber='555-1357', Address='654 Cedar St', City='San Francisco', State='CA', PostalCode=94102, Country='USA', DateOfBirth=datetime.date(1975, 12, 25)), Row(CustomerID=6, FirstName='Emily', LastName='Garcia', Email='emily.garcia@example.com', PhoneNumber='555-3698', Address='987 Birch St', City='Miami', State='FL', PostalCode=33101, Country='USA', DateOfBirth=datetime.date(1995, 4, 10)), Row(CustomerID=7, FirstName='David', LastName='Martinez', Email='david.martinez@example.com', PhoneNumber='555-7412', Address='258 Ash St', City='Denver', State='CO', PostalCode=80201, Country='USA', DateOfBirth=datetime.date(1987, 7, 23)), Row(CustomerID=8, FirstName='Olivia', LastName='Thompson', Email='olivia.thompson@example.com', PhoneNumber='555-8529', Address='753 Palm St', City='Phoenix', State='AZ', PostalCode=85001, Country='USA', DateOfBirth=datetime.date(1993, 2, 14)), Row(CustomerID=9, FirstName='Daniel', LastName='Anderson', Email='daniel.anderson@example.com', PhoneNumber='555-3692', Address='125 Willow St', City='New York', State='NY', PostalCode=10001, Country='USA', DateOfBirth=datetime.date(1980, 8, 30)), Row(CustomerID=10, FirstName='Sophia', LastName='Lopez', Email='sophia.lopez@example.com', PhoneNumber='555-9638', Address='789 Elm St', City='Los Angeles', State='CA', PostalCode=90001, Country='USA', DateOfBirth=datetime.date(1992, 5, 5))]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(r\"C:\\Users\\tharu\\Project_Git_Push\\New_Tharun_Branch\\Tharun_Projects\\Sample_Data\\data_1.csv\", header=True, inferSchema=True)\n",
    "rdd_4 = df.rdd\n",
    "print(type(rdd_4))\n",
    "print(rdd_4)\n",
    "print(rdd_4.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "empty_rdd = spark.sparkContext.emptyRDD()\n",
    "print(type(empty_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map\n",
    "Purpose: Transforms each element in the RDD using a function you provide.\n",
    "\n",
    "How it works: For each element in the RDD, the map function applies the given function and returns a new RDD with each element transformed.\n",
    "\n",
    "Output: The resulting RDD will have the same number of elements as the input RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'watsapp']\n",
      "['HELLO', 'WORLD', 'WATSAPP']\n"
     ]
    }
   ],
   "source": [
    "x = ['hello', 'world', 'watsapp']\n",
    "print(x)\n",
    "rdd_4 = spark.sparkContext.parallelize(x)\n",
    "rdd_5 = rdd_4.map(lambda x: x.upper())\n",
    "\n",
    "print(rdd_5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatmap\n",
    "Purpose: Transforms each element in the RDD using a function you provide and then flattens the results.\n",
    "\n",
    "How it works: For each element in the RDD, the flatMap function applies the given function, which should return an iterable. The results are then flattened into a single RDD.\n",
    "\n",
    "Output: The resulting RDD will contain the flattened elements from the iterables returned by the function. This means the output RDD might have more or fewer elements than the input RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'E', 'L', 'L', 'O', 'W', 'O', 'R', 'L', 'D', 'W', 'A', 'T', 'S', 'A', 'P', 'P']\n"
     ]
    }
   ],
   "source": [
    "rdd_6 = rdd_4.flatMap(lambda x: x.upper())\n",
    "print(rdd_6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "filter_rdd = rdd_1.filter(lambda x: x%2 == 0)\n",
    "print(filter_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 12, 14]\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 2, 3, 4], [6, 8, 9, 7, 5], [12, 13, 14]]  \n",
    "\n",
    "# Create an RDD from the list\n",
    "rdd_flatmap = spark.sparkContext.parallelize(a)\n",
    "\n",
    "# Flatten the list and filter only even numbers\n",
    "# Lambda function flattens the list and filters even numbers\n",
    "fm_rdd = rdd_flatmap.flatMap(lambda x: x).filter(lambda y: y % 2 == 0)\n",
    "\n",
    "# Collect the results and print them\n",
    "print(fm_rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world..!!,', 'Very', 'good', 'morning']\n"
     ]
    }
   ],
   "source": [
    "a = \"hello world..!!, Very good morning\"\n",
    "b = a.split(\" \")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, <pyspark.resultiterable.ResultIterable object at 0x000001B042A2AE10>), (20, <pyspark.resultiterable.ResultIterable object at 0x000001B042A40790>), (30, <pyspark.resultiterable.ResultIterable object at 0x000001B042A2B750>)]\n",
      "[(25, 5000), (20, 7000), (30, 4000)]\n"
     ]
    }
   ],
   "source": [
    "g = [(20, 2000), (25, 3000), (20, 5000), (30, 4000), (25, 2000)]\n",
    "g_rdd = spark.sparkContext.parallelize(g)\n",
    "g_rdd_1 = g_rdd.groupByKey()\n",
    "print(g_rdd_1.collect())\n",
    "g_rdd_2 = g_rdd_1.mapValues(lambda x: sum(x))\n",
    "print(g_rdd_2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 5000), (20, 7000), (30, 4000)]\n"
     ]
    }
   ],
   "source": [
    "g_rdd_3 = g_rdd.reduceByKey(lambda x,y : x+y)\n",
    "print(g_rdd_3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'test', 'a', 'sample', 'data', 'for', 'test', 'we', 'need', 'to', 'test', 'the', 'data', 'by', 'testing', 'data', 'with', 'data,', 'data', 'about', 'data', 'is', 'called', 'metadata', '.', 'reed', 'the', 'data', 'to', 'understand', 'the', 'data', 'and', 'the', 'type', 'of', 'data.']\n"
     ]
    }
   ],
   "source": [
    "text_rdd = spark.sparkContext.textFile(r\"C:\\Users\\tharu\\Project_Git_Push\\New_Tharun_Branch\\Tharun_Projects\\Sample_Data\\Text_file.txt\")\n",
    "rdd1 = text_rdd.flatMap(lambda x : x.split(\" \"))\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('To', 1), ('test', 1), ('a', 1), ('sample', 1), ('data', 1), ('for', 1), ('test', 1), ('we', 1), ('need', 1), ('to', 1), ('test', 1), ('the', 1), ('data', 1), ('by', 1), ('testing', 1), ('data', 1), ('with', 1), ('data,', 1), ('data', 1), ('about', 1), ('data', 1), ('is', 1), ('called', 1), ('metadata', 1), ('.', 1), ('reed', 1), ('the', 1), ('data', 1), ('to', 1), ('understand', 1), ('the', 1), ('data', 1), ('and', 1), ('the', 1), ('type', 1), ('of', 1), ('data.', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x : (x,1))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('To', 1), ('sample', 1), ('for', 1), ('to', 2), ('by', 1), ('testing', 1), ('with', 1), ('data,', 1), ('about', 1), ('called', 1), ('metadata', 1), ('.', 1), ('reed', 1), ('and', 1), ('type', 1), ('of', 1), ('data.', 1), ('test', 3), ('a', 1), ('data', 7), ('we', 1), ('need', 1), ('the', 4), ('is', 1), ('understand', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.reduceByKey(lambda x,y : x+y)\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
