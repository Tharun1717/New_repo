{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_3.5:0.20.3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `DateType()` can not accept object `nan` in type `float`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m     14\u001b[0m     StructField(name, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m pandas_df\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Assuming the last column is the date column\u001b[39;00m\n\u001b[0;32m     15\u001b[0m ] \u001b[38;5;241m+\u001b[39m [StructField(pandas_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], DateType(), \u001b[38;5;28;01mTrue\u001b[39;00m)])  \u001b[38;5;66;03m# Assuming the last column is the date column\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Step 3: Convert pandas DataFrame to PySpark DataFrame\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandas_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Show the PySpark DataFrame\u001b[39;00m\n\u001b[0;32m     21\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[1;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2157\u001b[0m             },\n\u001b[0;32m   2158\u001b[0m         )\n\u001b[0;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[1;32m-> 2160\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[1;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\types.py:2181\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_default\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_default\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2180\u001b[0m     assert_acceptable_types(obj)\n\u001b[1;32m-> 2181\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\types.py:2006\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2004\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[1;32m-> 2006\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   2007\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2008\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[0;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[0;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m   2012\u001b[0m             },\n\u001b[0;32m   2013\u001b[0m         )\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `DateType()` can not accept object `nan` in type `float`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\Test.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Step 2: Define the schema based on the Pandas DataFrame\n",
    "schema = StructType([\n",
    "    StructField(name, StringType(), True) for name in pandas_df.columns[:-1]  # Assuming the last column is the date column\n",
    "] + [StructField(pandas_df.columns[-1], DateType(), True)])  # Assuming the last column is the date column\n",
    "\n",
    "# Step 3: Convert pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df, schema=schema)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\u001b[39;00m\n\u001b[0;32m     16\u001b[0m filtered_pandas_df \u001b[38;5;241m=\u001b[39m pandas_df[filtered_columns]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m---> 17\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_pandas_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Show the PySpark DataFrame\u001b[39;00m\n\u001b[0;32m     20\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[0;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\conversion.py:362\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    360\u001b[0m             warn(msg)\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\sql\\pandas\\conversion.py:511\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[1;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[0;32m    504\u001b[0m             pdf[column] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[0;32m    505\u001b[0m                 ser\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mto_pytimedelta(), index\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    506\u001b[0m             )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Convert pandas.DataFrame to list of numpy records\u001b[39;00m\n\u001b[0;32m    509\u001b[0m np_records \u001b[38;5;241m=\u001b[39m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m--> 511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# Check if any columns need to be fixed for Spark to infer properly\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np_records) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:2558\u001b[0m, in \u001b[0;36mDataFrame.to_records\u001b[1;34m(self, index, column_dtypes, index_dtypes)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_mapping\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 2558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\core\\records.py:643\u001b[0m, in \u001b[0;36mfromarrays\u001b[1;34m(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\u001b[0m\n\u001b[0;32m    640\u001b[0m shape \u001b[38;5;241m=\u001b[39m _deprecate_shape_0_as_None(shape)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m     shape \u001b[38;5;241m=\u001b[39m \u001b[43marrayList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(shape, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    645\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (shape,)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\Test.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Step 2: Exclude columns with the name \"Unnamed\"\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\n",
    "filtered_pandas_df = pandas_df[filtered_columns].dropna()\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+\n",
      "|        Customer| Hierarchy|    ParentH1|                Site| Commitment Type|     Contract Start|Contract End|Summary Year|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|           Allentown|Pallet Positions|2016-10-31 00:00:00|    9/1/2026|        2026|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Ft. Worth - Railhead|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|           Massillon|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Massillon - Erie Ave|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path, usecols=[\"Customer\", \"Hierarchy\", \"ParentH1\", \"Site\", \"Commitment Type\", \"Contract Start\", \"Contract End\", \"Summary Year\"])\n",
    "\n",
    "# Step 2: Exclude columns with the name \"Unnamed\"\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\n",
    "filtered_pandas_df = pandas_df[filtered_columns].dropna()\n",
    "\n",
    "# Step 4: Convert 'Contract Start' column to PySpark DateType\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "filtered_pandas_df = filtered_pandas_df.dropna(subset=['Contract Start'])\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to C:\\Users\\tharu\\output_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Allentown\", \"Pallet Positions\", \"10/31/2016\", \"9/1/2026\", 2026, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 5200, 7500, 7500, 7500, 7500, 5967],\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Ft. Worth - Railhead\", \"Pallet Positions\", \"7/1/2016\", \"9/1/2026\", 2026, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 9600, 10000, 10000, 10000, 10000, 9733],\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Massillon\", \"Pallet Positions\", \"7/1/2016\", \"9/1/2026\", 2026, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596, 19596],\n",
    "    [\"Kraft Heinz Corp\", \"KRAFTHEINZ\", \"KRAFTHEINZ_W\", \"Massillon - Erie Ave\", \"Pallet Positions\", \"7/1/2016\", \"9/1/2026\", 2026, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11577, 11647, 11647, 11647, 11647, 11600]\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"Customer\", \"Hierarchy\", \"ParentH1\", \"Site\", \"Commitment Type\", \"Contract Start\", \"Contract End\", \"Summary Year\"] + \\\n",
    "          [f\"Jan {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Feb {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Mar {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Apr {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"May {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Jun {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Jul {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Aug {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Sep {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Oct {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Nov {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Dec {year}\" for year in range(2022, 2024)] + \\\n",
    "          [f\"Average {year}\" for year in range(2022, 2024)]\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Writing to an Excel file using a raw string\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been written to {excel_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "|        Customer| Hierarchy|    ParentH1|                Site| Commitment Type|     Contract Start|Contract End|Summary Year|Jan 2022|Jan 2023|Feb 2022|Feb 2023|Mar 2022|Mar 2023|Apr 2022|Apr 2023|May 2022|May 2023|Jun 2022|Jun 2023|Jul 2022|Jul 2023|Aug 2022|Aug 2023|Sep 2022|Sep 2023|Oct 2022|Oct 2023|Nov 2022|Nov 2023|Dec 2022|Dec 2023|Average 2022|Average 2023|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Massillon - Erie Ave|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577| 11577.0| 11647.0| 11647.0| 11647.0|     11647.0|     11600.0|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "\n",
    "# Skip the first 5 rows (0-indexed) and read the data\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and convert to PySpark DataFrame\n",
    "filtered_pandas_df = pandas_df[filtered_columns].dropna()\n",
    "\n",
    "# Step 4: Convert 'Contract Start' column to PySpark DateType (Assuming 'Contract Start' is the correct date column)\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "\n",
    "filtered_pandas_df = filtered_pandas_df.dropna(subset=['Contract Start'])\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "|        Customer| Hierarchy|    ParentH1|                Site| Commitment Type|     Contract Start|Contract End|Summary Year|Jan 2022|Jan 2023|Feb 2022|Feb 2023|Mar 2022|Mar 2023|Apr 2022|Apr 2023|May 2022|May 2023|Jun 2022|Jun 2023|Jul 2022|Jul 2023|Aug 2022|Aug 2023|Sep 2022|Sep 2023|Oct 2022|Oct 2023|Nov 2022|Nov 2023|Dec 2022|Dec 2023|Average 2022|Average 2023|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|           Allentown|Pallet Positions|2016-10-31 00:00:00|    9/1/2026|        2026|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    7500|    7500|    7500|    7500|    5967|       -|       -|       -|       -|           -|           -|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Ft. Worth - Railhead|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|   10000|   10000|   10000|   10000|    9733|       -|       -|       -|       -|           -|           -|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|           Massillon|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596| 19596.0| 19596.0| 19596.0|       -|           -|           -|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Massillon - Erie Ave|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577| 11577.0| 11647.0| 11647.0| 11647.0|     11647.0|     11600.0|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "\n",
    "# Skip the first 5 rows (0-indexed) and read the data\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and replace null values with '-'\n",
    "filtered_pandas_df = pandas_df[filtered_columns].fillna('-')\n",
    "\n",
    "# Step 4: Convert 'Contract Start' column to PySpark DateType (Assuming 'Contract Start' is the correct date column)\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "\n",
    "filtered_pandas_df = filtered_pandas_df.dropna(subset=['Contract Start'])\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "|        Customer| Hierarchy|    ParentH1|                Site| Commitment Type|     Contract Start|Contract End|Summary Year|Jan 2022|Jan 2023|Feb 2022|Feb 2023|Mar 2022|Mar 2023|Apr 2022|Apr 2023|May 2022|May 2023|Jun 2022|Jun 2023|Jul 2022|Jul 2023|Aug 2022|Aug 2023|Sep 2022|Sep 2023|Oct 2022|Oct 2023|Nov 2022|Nov 2023|Dec 2022|Dec 2023|Average 2022|Average 2023|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|           Allentown|Pallet Positions|2016-10-31 00:00:00|    9/1/2026|        2026|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    5200|    7500|    7500|    7500|    7500|    5967|       -|       -|       -|       -|           -|           -|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Ft. Worth - Railhead|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|    9600|   10000|   10000|   10000|   10000|    9733|       -|       -|       -|       -|           -|           -|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|           Massillon|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596|   19596| 19596.0| 19596.0| 19596.0|       -|           -|           -|\n",
      "|Kraft Heinz Corp|KRAFTHEINZ|KRAFTHEINZ_W|Massillon - Erie Ave|Pallet Positions|2016-07-01 00:00:00|    9/1/2026|        2026|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577|   11577| 11577.0| 11647.0| 11647.0| 11647.0|     11647.0|     11600.0|\n",
      "+----------------+----------+------------+--------------------+----------------+-------------------+------------+------------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------------+------------+\n",
      "\n",
      "Data has been written to C:\\Users\\tharu\\output_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "filtered_pandas_df = pandas_df[filtered_columns].fillna('-')\n",
    "\n",
    "# Step 3: Convert 'Contract Start' column to PySpark DateType (Assuming 'Contract Start' is the correct date column)\n",
    "filtered_pandas_df['Contract Start'] = pd.to_datetime(filtered_pandas_df['Contract Start'], errors='coerce')\n",
    "\n",
    "# Step 4: Replace null values with '-'\n",
    "filtered_pandas_df = filtered_pandas_df.fillna('-')\n",
    "\n",
    "# Step 5: Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(filtered_pandas_df)\n",
    "\n",
    "# Show the PySpark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "# Step 6: Write to CSV file\n",
    "csv_output_path = r\"C:\\Users\\tharu\\output_data.csv\"\n",
    "filtered_pandas_df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"Data has been written to {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Customer   Hierarchy      ParentH1                  Site  \\\n",
      "0  Kraft Heinz Corp  KRAFTHEINZ  KRAFTHEINZ_W             Allentown   \n",
      "1  Kraft Heinz Corp  KRAFTHEINZ  KRAFTHEINZ_W  Ft. Worth - Railhead   \n",
      "2  Kraft Heinz Corp  KRAFTHEINZ  KRAFTHEINZ_W             Massillon   \n",
      "3  Kraft Heinz Corp  KRAFTHEINZ  KRAFTHEINZ_W  Massillon - Erie Ave   \n",
      "\n",
      "    Commitment Type Contract Start Contract End  Summary Year  Jan 2022  \\\n",
      "0  Pallet Positions     10/31/2016     9/1/2026          2026      5200   \n",
      "1  Pallet Positions       7/1/2016     9/1/2026          2026      9600   \n",
      "2  Pallet Positions       7/1/2016     9/1/2026          2026     19596   \n",
      "3  Pallet Positions       7/1/2016     9/1/2026          2026     11577   \n",
      "\n",
      "   Jan 2023  ...  Sep 2022  Sep 2023  Oct 2022  Oct 2023  Nov 2022  Nov 2023  \\\n",
      "0      5200  ...      7500      7500      7500      5967         -         -   \n",
      "1      9600  ...     10000     10000     10000      9733         -         -   \n",
      "2     19596  ...     19596     19596     19596     19596   19596.0   19596.0   \n",
      "3     11577  ...     11577     11577     11577     11577   11577.0   11647.0   \n",
      "\n",
      "   Dec 2022  Dec 2023  Average 2022  Average 2023  \n",
      "0         -         -             -             -  \n",
      "1         -         -             -             -  \n",
      "2   19596.0         -             -             -  \n",
      "3   11647.0   11647.0       11647.0       11600.0  \n",
      "\n",
      "[4 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Step 1: Read Excel file with pandas\n",
    "excel_file_path = r\"C:\\Users\\tharu\\output_data.xlsx\"\n",
    "\n",
    "# Skip the first 5 rows (0-indexed) and read the data\n",
    "pandas_df = pd.read_excel(excel_file_path, skiprows=5)\n",
    "\n",
    "# Step 2: Remove empty columns (Unnamed)\n",
    "filtered_columns = [col for col in pandas_df.columns if \"Unnamed\" not in col]\n",
    "\n",
    "# Step 3: Filter Pandas DataFrame and replace null values with '-'\n",
    "filtered_pandas_df = pandas_df[filtered_columns].fillna('-')\n",
    "\n",
    "print(filtered_pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provided data\n",
    "data = {\n",
    "    'Customer': ['Kraft Heinz Corp'] * 8 + ['ConAgra Foods Inc'] * 7 + ['Sara Lee'] + ['Lamb Weston'] * 2,\n",
    "    'Hierarchy': ['KRAFTHEINZ'] * 8 + ['CONAGRAPINNACLE'] * 7 + ['SARALEE'] + ['LAMB'] * 2,\n",
    "    'ParentH1': ['KRAFTHEINZ_W'] * 8 + ['CONAGRAPINNACLE_W'] * 7 + ['SARALEE_W'] + ['LAMB_W'] * 2,\n",
    "    'Site': ['Allentown', 'Ft. Worth - Railhead', 'Massillon', 'Massillon - Erie Ave', 'Nampa',\n",
    "             'Ontario OR', 'Rochelle', 'Westgate', 'Atlanta - East Point', 'Carthage', 'Indianapolis',\n",
    "             'Manchester', 'Rochelle Caron', 'Russellville ElMira', 'Victorville', 'Tarboro', 'Westgate', 'Delhi', None],\n",
    "    'Commitment Type': ['Pallet Positions'] * 19\n",
    "}\n",
    "\n",
    "# Ensure all arrays have the same length\n",
    "max_length = max(len(arr) for arr in data.values())\n",
    "data = {key: arr + [None] * (max_length - len(arr)) for key, arr in data.items()}\n",
    "\n",
    "# Schema\n",
    "schema = ['Customer', 'Hierarchy', 'ParentH1', 'Site', 'Commitment Type']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data, columns=schema)\n",
    "\n",
    "# Save DataFrame to Excel file with schema\n",
    "df.to_excel(r'C:\\Users\\tharu\\output_data_1.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Customer        Hierarchy           ParentH1  \\\n",
      "0    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "1    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "2    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "3    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "4    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "5    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "6    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "7    Kraft Heinz Corp       KRAFTHEINZ       KRAFTHEINZ_W   \n",
      "8   ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "9   ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "10  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "11  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "12  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "13  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "14  ConAgra Foods Inc  CONAGRAPINNACLE  CONAGRAPINNACLE_W   \n",
      "15           Sara Lee          SARALEE          SARALEE_W   \n",
      "16        Lamb Weston             LAMB             LAMB_W   \n",
      "17        Lamb Weston             LAMB             LAMB_W   \n",
      "18                NaN              NaN                NaN   \n",
      "\n",
      "                    Site   Commitment Type  \n",
      "0              Allentown  Pallet Positions  \n",
      "1   Ft. Worth - Railhead  Pallet Positions  \n",
      "2              Massillon  Pallet Positions  \n",
      "3   Massillon - Erie Ave  Pallet Positions  \n",
      "4                  Nampa  Pallet Positions  \n",
      "5             Ontario OR  Pallet Positions  \n",
      "6               Rochelle  Pallet Positions  \n",
      "7               Westgate  Pallet Positions  \n",
      "8   Atlanta - East Point  Pallet Positions  \n",
      "9               Carthage  Pallet Positions  \n",
      "10          Indianapolis  Pallet Positions  \n",
      "11            Manchester  Pallet Positions  \n",
      "12        Rochelle Caron  Pallet Positions  \n",
      "13   Russellville ElMira  Pallet Positions  \n",
      "14           Victorville  Pallet Positions  \n",
      "15               Tarboro  Pallet Positions  \n",
      "16              Westgate  Pallet Positions  \n",
      "17                 Delhi  Pallet Positions  \n",
      "18                   NaN  Pallet Positions  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file\n",
    "df_read = pd.read_excel(r'C:\\Users\\tharu\\output_data_1.xlsx')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_read)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
